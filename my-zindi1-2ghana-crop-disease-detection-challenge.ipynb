{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9552110,"sourceType":"datasetVersion","datasetId":5820118},{"sourceId":9690307,"sourceType":"datasetVersion","datasetId":5924189}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/ghana-crop-disease-detection-data/Train.csv')\nunique_classes = df['class'].unique()\nclass_mapping = {cls: idx for idx, cls in enumerate(unique_classes)}\nprint(class_mapping)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T16:43:13.939379Z","iopub.execute_input":"2024-10-29T16:43:13.940132Z","iopub.status.idle":"2024-10-29T16:43:15.044673Z","shell.execute_reply.started":"2024-10-29T16:43:13.940097Z","shell.execute_reply":"2024-10-29T16:43:15.043660Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"{'Pepper_Bacterial_Spot': 0, 'Pepper_Fusarium': 1, 'Corn_Cercospora_Leaf_Spot': 2, 'Corn_Common_Rust': 3, 'Tomato_Early_Blight': 4, 'Pepper_Septoria': 5, 'Tomato_Septoria': 6, 'Pepper_Leaf_Curl': 7, 'Pepper_Leaf_Mosaic': 8, 'Corn_Streak': 9, 'Corn_Healthy': 10, 'Pepper_Healthy': 11, 'Tomato_Healthy': 12, 'Pepper_Late_Blight': 13, 'Tomato_Late_Blight': 14, 'Pepper_Cercospora': 15, 'Tomato_Fusarium': 16, 'Pepper_Leaf_Blight': 17, 'Tomato_Leaf_Curl': 18, 'Tomato_Bacterial_Spot': 19, 'Tomato_Mosaic': 20, 'Pepper_Early_Blight': 21, 'Corn_Northern_Leaf_Blight': 22}\n","output_type":"stream"}]},{"cell_type":"code","source":"# import nbformat\n\n# # Path to the notebook\n# notebook_path = '/kaggle/input/ghana-csv-data-starter-notebook/Rail_Challenge_Starter.ipynb'  # Update this with the correct path\n\n# # Load the notebook\n# with open(notebook_path, 'r') as f:\n#     notebook_content = nbformat.read(f, as_version=4)\n\n# # Extract code cells\n# code_cells = [cell['source'] for cell in notebook_content.cells if cell.cell_type == 'code']\n\n# # Print the code cells\n# for i, code in enumerate(code_cells):\n#     print(f\"Code Cell {i + 1}:\\n{code}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T23:37:02.051791Z","iopub.execute_input":"2024-10-26T23:37:02.052311Z","iopub.status.idle":"2024-10-26T23:37:02.059364Z","shell.execute_reply.started":"2024-10-26T23:37:02.052253Z","shell.execute_reply":"2024-10-26T23:37:02.05736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import libraries\n!pip install ultralytics\nimport pandas as pd\nimport os\nfrom pathlib import Path\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport cv2\nimport yaml\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nimport multiprocessing\n\n\n\n# Set the data directory\nDATA_DIR = Path('/kaggle/input/ghana-crop-disease-detection-data')\n\n# Load train and test files\ntrain = pd.read_csv(DATA_DIR / 'Train.csv')\ntest = pd.read_csv(DATA_DIR / 'Test.csv')\nss = pd.read_csv(DATA_DIR / 'SampleSubmission.csv')\n\n# Add an image_path column\ntrain['image_path'] = [Path('/kaggle/input/ghana-crop-disease-detection-data/images/' + x) for x in train.Image_ID]\ntest['image_path'] = [Path('/kaggle/input/ghana-crop-disease-detection-data/images/' + x) for x in test.Image_ID]\n\n# Map string classes to integers (label encoding targets)\ntrain['class_id'] = train['class'].map(\n    {'Pepper_Bacterial_Spot': 0, 'Pepper_Fusarium': 1, 'Corn_Cercospora_Leaf_Spot': 2, 'Corn_Common_Rust': 3, 'Tomato_Early_Blight': 4, 'Pepper_Septoria': 5, 'Tomato_Septoria': 6, 'Pepper_Leaf_Curl': 7, 'Pepper_Leaf_Mosaic': 8, 'Corn_Streak': 9, 'Corn_Healthy': 10, 'Pepper_Healthy': 11, 'Tomato_Healthy': 12, 'Pepper_Late_Blight': 13, 'Tomato_Late_Blight': 14, 'Pepper_Cercospora': 15, 'Tomato_Fusarium': 16, 'Pepper_Leaf_Blight': 17, 'Tomato_Leaf_Curl': 18, 'Tomato_Bacterial_Spot': 19, 'Tomato_Mosaic': 20, 'Pepper_Early_Blight': 21, 'Corn_Northern_Leaf_Blight': 22})\n\n# Split data into training and validation\ntrain_unique_imgs_df = train.drop_duplicates(subset=['Image_ID'], ignore_index=True)\nX_train, X_val = train_test_split(train_unique_imgs_df, test_size=0.25, stratify=train_unique_imgs_df['class'], random_state=42)\n\nX_train = train[train.Image_ID.isin(X_train.Image_ID)]\nX_val = train[train.Image_ID.isin(X_val.Image_ID)]\n\n# Check the shapes of training and validation data\nprint(X_train.shape, X_val.shape)\n\n# Define directories for images and labels\nTRAIN_IMAGES_DIR = Path('/kaggle/working/train/images')\nVAL_IMAGES_DIR = Path('/kaggle/working/val/images')\nTEST_IMAGES_DIR = Path('/kaggle/working/test/images')\nTRAIN_LABELS_DIR = Path('/kaggle/working/train/labels')\nVAL_LABELS_DIR = Path('/kaggle/working/val/labels')\nTEST_LABELS_DIR = Path('/kaggle/working/test/labels')\n\n# Create necessary directories\nfor DIR in [TRAIN_IMAGES_DIR, VAL_IMAGES_DIR, TEST_IMAGES_DIR, TRAIN_LABELS_DIR, VAL_LABELS_DIR, TEST_LABELS_DIR]:\n    if DIR.exists():\n        shutil.rmtree(DIR)\n    DIR.mkdir(parents=True, exist_ok=True)\n\n# Copy train, val, and test images to their respective dirs\nfor img in tqdm(X_train.image_path.unique()):\n    shutil.copy(img, TRAIN_IMAGES_DIR / img.parts[-1])\n\nfor img in tqdm(X_val.image_path.unique()):\n    shutil.copy(img, VAL_IMAGES_DIR / img.parts[-1])\n\nfor img in tqdm(test.image_path.unique()):\n    shutil.copy(img, TEST_IMAGES_DIR / img.parts[-1])\n\n# Function to convert the bounding boxes to YOLO format and save them\ndef save_yolo_annotation(row):\n    image_path, class_id, output_dir = row['image_path'], row['class_id'], row['output_dir']\n\n    img = cv2.imread(str(image_path))\n    if img is None:\n        raise ValueError(f\"Could not read image from path: {image_path}\")\n\n    height, width, _ = img.shape\n    label_file = Path(output_dir) / f\"{Path(image_path).stem}.txt\"\n\n    ymin, xmin, ymax, xmax = row['ymin'], row['xmin'], row['ymax'], row['xmax']\n\n    # Normalize the coordinates\n    x_center = (xmin + xmax) / 2 / width\n    y_center = (ymin + ymax) / 2 / height\n    bbox_width = (xmax - xmin) / width\n    bbox_height = (ymax - ymin) / height\n\n    with open(label_file, 'a') as f:\n        f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n\n# Parallelize the annotation saving process\ndef process_dataset(dataframe, output_dir):\n    dataframe['output_dir'] = output_dir\n    with multiprocessing.Pool() as pool:\n        list(tqdm(pool.imap(save_yolo_annotation, dataframe.to_dict('records')), total=len(dataframe)))\n\n# Save train and validation labels to their respective dirs\nprocess_dataset(X_train, TRAIN_LABELS_DIR)\nprocess_dataset(X_val, VAL_LABELS_DIR)\n\n# Create a data.yaml file required by YOLO\nclass_names = train['class'].unique().tolist()\nnum_classes = len(class_names)\n\ndata_yaml = {\n    'train': str(TRAIN_IMAGES_DIR),\n    'val': str(VAL_IMAGES_DIR),\n    'nc': num_classes,\n    'names': class_names\n}\n\n# Save the data.yaml file\nyaml_path = Path('data.yaml')\nwith open(yaml_path, 'w') as file:\n    yaml.dump(data_yaml, file, default_flow_style=False)\n\n# Load a YOLO pretrained model\nmodel = YOLO('yolov8s.pt')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T16:43:34.619929Z","iopub.execute_input":"2024-10-29T16:43:34.620454Z","iopub.status.idle":"2024-10-29T17:05:24.350342Z","shell.execute_reply.started":"2024-10-29T16:43:34.620409Z","shell.execute_reply":"2024-10-29T17:05:24.349122Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.3.24-py3-none-any.whl.metadata (35 kB)\nRequirement already satisfied: numpy>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (10.3.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.14.1)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.4.0)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.19.0)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.2.2)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.9-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\nDownloading ultralytics-8.3.24-py3-none-any.whl (877 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m877.7/877.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.9-py3-none-any.whl (26 kB)\nInstalling collected packages: ultralytics-thop, ultralytics\nSuccessfully installed ultralytics-8.3.24 ultralytics-thop-2.0.9\nCreating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n(30777, 9) (10252, 9)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3676 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63829d3841ef457bbfb6968e2a255241"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1226 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e164906441a41f0988efdb9396c6156"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7a7208aa59d4adeadd1a7472c3be69a"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/1866894355.py:91: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dataframe['output_dir'] = output_dir\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30777 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1e4b323f3b9411ba727dd211f94fc2e"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/1866894355.py:91: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dataframe['output_dir'] = output_dir\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10252 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80fd3c716af244b2a51f479e201120e4"}},"metadata":{}},{"name":"stdout","text":"Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 21.5M/21.5M [00:00<00:00, 145MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"model.train(\n    data='data.yaml',          # Path to the dataset configuration\n    epochs=10,                 # Number of epochs\n    imgsz=540,               # Image size\n    batch=16,                  # Batch size\n    device='cpu',               # Use the first GPU (0 for the first GPU)\n    patience=3                # Number of epochs with no improvement after which training will stop\n)\n\n# Validate the model on the validation seta\nmodel.val()\n\n# Make predictions on test images and save them to a CSV file\nimage_files = os.listdir(TEST_IMAGES_DIR)\nall_data = []\n\nfor image_file in tqdm(image_files):\n    img_path = os.path.join(TEST_IMAGES_DIR, image_file)\n    results = model(img_path)\n\n    boxes = results[0].boxes.xyxy.tolist()\n    classes = results[0].boxes.cls.tolist()\n    confidences = results[0].boxes.conf.tolist()\n    names = results[0].names\n    \n    incorrect_prediction = 0 \n    \n    if not boxes:\n        incorrect_prediction = incorrect_prediction +   1 \n        all_data.append({\n            'Image_ID': image_file,\n            'class': 'NEG',\n            'confidence': 1.0,\n            'ymin': 0,\n            'xmin': 0,\n            'ymax': 0,\n            'xmax': 0\n        })\n    else:\n        for box, cls, conf in zip(boxes, classes, confidences):\n            x1, y1, x2, y2 = box\n            detected_class = names[int(cls)]\n\n            all_data.append({\n                'Image_ID': image_file,\n                'class': detected_class,\n                'confidence': conf,\n                'ymin': y1,\n                'xmin': x1,\n                'ymax': y2,\n                'xmax': x2\n            })\nprint(f'========== Total Incorrect predction or dummy prediction {incorrect_prediction}================')\n# Convert the results to a DataFrame and save it\nsub = pd.DataFrame(all_data)\nsub.to_csv('/kaggle/working/benchmark_submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:05:39.067628Z","iopub.execute_input":"2024-10-29T17:05:39.068047Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Ultralytics 8.3.24 🚀 Python-3.10.14 torch-2.4.0 CPU (Intel Xeon 2.00GHz)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=data.yaml, epochs=10, time=None, patience=3, batch=16, imgsz=540, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 755k/755k [00:00<00:00, 11.4MB/s]\n2024-10-29 17:05:42,018\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n2024-10-29 17:05:42,568\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n","output_type":"stream"},{"name":"stdout","text":"Overriding model.yaml nc=80 with nc=23\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n 22        [15, 18, 21]  1   2124949  ultralytics.nn.modules.head.Detect           [23, [128, 256, 512]]         \nModel summary: 225 layers, 11,144,501 parameters, 11,144,485 gradients, 28.7 GFLOPs\n\nTransferred 349/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\nWARNING ⚠️ imgsz=[540] must be multiple of max stride 32, updating to [544]\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/train/labels... 3676 images, 0 backgrounds, 0 corrupt: 100%|██████████| 3676/3676 [00:05<00:00, 699.00it/s] \n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/train/labels.cache\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.20 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/val/labels... 1226 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1226/1226 [00:01<00:00, 1128.79it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/val/labels.cache\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to runs/detect/train/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00037, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\nImage sizes 544 train, 544 val\nUsing 0 dataloader workers\nLogging results to \u001b[1mruns/detect/train\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10         0G      2.759      4.983      1.983        119        544:  77%|███████▋  | 176/230 [28:22<08:30,  9.45s/it]","output_type":"stream"}]},{"cell_type":"code","source":"#apikey for model acessing\n# 928e0393be6ece8cbbd9a973386809288cb48eac\n\n\"\"\"\nDiscussion Zindi ::\nStater Notebook Ghana Crop Disease Detection\nI have used used   model = YOLO('yolov8m.pt')   pretrained model for refinding on my data sets I have ajust the basic code to refined this model and make submission file. You have to input api key you can get  from this website https://wandb.ai/site/  singup or login if have account  after copy  apikey from   which is used required in accessing pretrained model of YOLO While runing the code.       Here is link  of  Notebook on kaggle () \n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-27T00:04:34.679266Z","iopub.status.idle":"2024-10-27T00:04:34.67978Z","shell.execute_reply.started":"2024-10-27T00:04:34.679531Z","shell.execute_reply":"2024-10-27T00:04:34.679553Z"},"trusted":true},"execution_count":null,"outputs":[]}]}